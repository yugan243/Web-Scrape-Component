{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8df435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q httpx[http2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c2df32",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c3823e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "import httpx\n",
    "import re\n",
    "import time\n",
    "from selectolax.parser import HTMLParser # Using the faster selectolax parser\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional, Set, Any\n",
    "from tqdm.asyncio import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfa0585",
   "metadata": {},
   "source": [
    "### 2. Scraping Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dee86cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncLaptopLKScraper:\n",
    "    def __init__(self, max_connections: int = 25, max_retries: int = 3):\n",
    "        self.source_website = \"laptop.lk\"\n",
    "        self.scrape_timestamp = datetime.now().isoformat()\n",
    "        self.shop_phone = \"+94 77 733 6464\"\n",
    "        self.shop_whatsapp = \"+94 77 733 6464\"\n",
    "        self.max_retries = max_retries\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        self.semaphore = asyncio.Semaphore(max_connections)\n",
    "\n",
    "    async def fetch_page(self, client: httpx.AsyncClient, url: str) -> Optional[str]:\n",
    "        async with self.semaphore:\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    response = await client.get(url, headers=self.headers, timeout=30, follow_redirects=True)\n",
    "                    response.raise_for_status()\n",
    "                    return response.text\n",
    "                except (httpx.RequestError, httpx.HTTPStatusError):\n",
    "                    if attempt + 1 == self.max_retries: break\n",
    "                    await asyncio.sleep(2 ** attempt)\n",
    "        return None\n",
    "\n",
    "    # --- MODIFIED: This function now uses the much faster selectolax parser ---\n",
    "    def parse_product_data(self, html: str, url: str) -> Optional[Dict]:\n",
    "        try:\n",
    "            tree = HTMLParser(html)\n",
    "            product_container = tree.css_first(\"div[id^=product-]\")\n",
    "            if not product_container: return None\n",
    "\n",
    "            # Decompose is not available; we just select from the container\n",
    "            \n",
    "            title_node = product_container.css_first(\"h1.product_title\")\n",
    "            title = title_node.text(strip=True) if title_node else None\n",
    "\n",
    "            product_id = product_container.id.split('-')[-1] if product_container.id else None\n",
    "            \n",
    "            desc_node = product_container.css_first(\"div#tab-description, div.woocommerce-tabs\")\n",
    "            description_html = desc_node.html if desc_node else None\n",
    "\n",
    "            category_nodes = product_container.css(\"span.posted_in a\")\n",
    "            all_categories = [node.text(strip=True) for node in category_nodes]\n",
    "            brand = next((cat for cat in all_categories if cat.lower() in ['hp', 'dell', 'apple', 'lenovo', 'asus', 'msi', 'acer', 'samsung']), None)\n",
    "            category_path = [c for c in all_categories if c.lower() != (brand or '').lower()]\n",
    "\n",
    "            image_nodes = product_container.css(\"div.woocommerce-product-gallery__image a\")\n",
    "            image_urls = [node.attributes.get('href') for node in image_nodes]\n",
    "\n",
    "            price_curr_node = product_container.css_first(\"p.price ins .amount, span.electro-price ins .amount, p.price > .amount, span.electro-price > .amount\")\n",
    "            price_orig_node = product_container.css_first(\"p.price del .amount, span.electro-price del .amount\")\n",
    "            price_current = re.sub(r'[^\\d.]', '', price_curr_node.text(strip=True)) if price_curr_node else \"0\"\n",
    "            price_original = re.sub(r'[^\\d.]', '', price_orig_node.text(strip=True)) if price_orig_node else None\n",
    "\n",
    "            availability_text = \"Out of Stock\" if product_container.css_first(\"p.stock.out-of-stock\") else \"In Stock\"\n",
    "\n",
    "            warranty_text = None\n",
    "            warranty_img = product_container.css_first(\"img[alt*='warranty' i]\")\n",
    "            if warranty_img and 'alt' in warranty_img.attributes:\n",
    "                warranty_text = warranty_img.attributes['alt'].replace('Year-warranty', ' Year Warranty').replace('-', ' ')\n",
    "\n",
    "            variants = [{\"variant_id_native\": product_id, \"variant_title\": \"Default\", \"price_current\": price_current, \"price_original\": price_original, \"currency\": \"LKR\", \"availability_text\": availability_text}]\n",
    "            \n",
    "            return {\"product_id_native\": product_id, \"product_url\": url, \"product_title\": title, \"warranty\": warranty_text,\"description_html\": description_html, \"brand\": brand, \"category_path\": category_path, \"image_urls\": image_urls, \"variants\": variants, \"metadata\": {\"source_website\": self.source_website, \"shop_contact_phone\": self.shop_phone,\"shop_contact_whatsapp\": self.shop_whatsapp, \"scrape_timestamp\": self.scrape_timestamp}}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def save_data(self, data: List[Dict[str, Any]], filename: str):\n",
    "        output = {\"extraction_info\": {\"total_products_extracted\": len(data), \"extraction_timestamp\": self.scrape_timestamp}, \"products\": data}\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"\\nâœ… Data successfully saved to {filename}\")\n",
    "\n",
    "# Helper function for the optimized pipeline\n",
    "async def fetch_and_parse_product(client: httpx.AsyncClient, scraper: AsyncLaptopLKScraper, url: str) -> Optional[Dict]:\n",
    "    html = await scraper.fetch_page(client, url)\n",
    "    if html:\n",
    "        return scraper.parse_product_data(html, url)\n",
    "    return None\n",
    "\n",
    "# The main function orchestrating the entire scrape\n",
    "async def main() -> int:\n",
    "    scraper = AsyncLaptopLKScraper()\n",
    "    sitemap_index_url = \"https://www.laptop.lk/sitemap_index.xml\"\n",
    "\n",
    "    async with httpx.AsyncClient(http2=True) as client: # Enabling HTTP/2 for potential speed up\n",
    "        print(f\"--- Fetching sitemap index: {sitemap_index_url} ---\")\n",
    "        index_xml = await scraper.fetch_page(client, sitemap_index_url)\n",
    "        if not index_xml: return 0\n",
    "\n",
    "        product_sitemap_urls = [node.text() for node in HTMLParser(index_xml).css('loc') if 'product-sitemap' in node.text()]\n",
    "        \n",
    "        sitemap_tasks = [scraper.fetch_page(client, url) for url in product_sitemap_urls]\n",
    "        sitemap_xmls = await asyncio.gather(*sitemap_tasks)\n",
    "        \n",
    "        unique_product_urls = {loc.text() for xml in sitemap_xmls if xml for loc in HTMLParser(xml).css('url > loc')}\n",
    "        product_urls_list = list(unique_product_urls)\n",
    "        \n",
    "        print(f\"\\nFound {len(product_urls_list)} unique product URLs to scrape.\")\n",
    "        if not product_urls_list: return 0\n",
    "\n",
    "        print(f\"--- Scraping {len(product_urls_list)} products ---\")\n",
    "        tasks = [fetch_and_parse_product(client, scraper, url) for url in product_urls_list]\n",
    "        results = await tqdm.gather(*tasks, desc=\"Scraping Products\")\n",
    "        all_products_data = [item for item in results if item is not None]\n",
    "\n",
    "    scraper.save_data(all_products_data, \"..\\RawData\\laptop_lk_scrape.json\")\n",
    "    return len(all_products_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51390f44",
   "metadata": {},
   "source": [
    "### 3. Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22efc76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the main scraper function and get the final count\n",
    "product_count = await main()\n",
    "\n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the performance summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸš€ SCRAPE PERFORMANCE SUMMARY ðŸš€\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Products Scraped: {product_count}\")\n",
    "print(f\"Total Time Taken: {end_time - start_time:.2f} seconds\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
